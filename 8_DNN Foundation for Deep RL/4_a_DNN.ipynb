{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wSum(X,W):\n",
    "    h = torch.from_numpy(X)\n",
    "    z = torch.matmul(W,h)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(x):\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardStep(X,W_list):\n",
    "    h = torch.from_numpy(X)\n",
    "    for W in W_list:\n",
    "        z = torch.matmul(W,h)\n",
    "        h = activate(z)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(W_list,dW_list,lr):\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(W_list)):\n",
    "            W_list[i] -= lr*dW_list[i]\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_sgd(X,y,W_list,loss_fn,lr=0.0001,nepochs=100):\n",
    "    for epoch in range(nepochs):\n",
    "        avgLoss = []\n",
    "        for i in range(len(y)):\n",
    "            Xin = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forwardStep(Xin,W_list)\n",
    "            loss = loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "            loss.backward()\n",
    "            avgLoss.append(loss.item())\n",
    "            sys.stdout.flush()\n",
    "            dW_list = []\n",
    "            for j in range(len(W_list)):\n",
    "                dW_list.append(W_list[j].grad.data)\n",
    "            W_list = updateParams(W_list,dW_list,lr)\n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,np.mean(np.array(avgLoss))))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_batch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100):\n",
    "    n = len(y) # Compute number\n",
    "    for epoch in range(nepochs):\n",
    "        loss = 0\n",
    "        for i in range(n):\n",
    "            Xin = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forwardStep(Xin,W_list)\n",
    "            # Compute Total Loss Then we update.\n",
    "            loss += loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "        loss = loss/n  # Average loss of the epoch.\n",
    "        loss.backward() \n",
    "        sys.stdout.flush()\n",
    "        dW_list = []\n",
    "        for j in range(len(W_list)):\n",
    "            dW_list.append(W_list[j].grad.data)\n",
    "        W_list = updateParams(W_list,dW_list,lr)\n",
    "        for j in range(len(W_list)):\n",
    "            W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_minibatch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100,batchSize=16):\n",
    "    n = len(y)\n",
    "    numBatches = n//batchSize\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for batch in range(numBatches):\n",
    "            X_batch = X[batch*batchSize:(batch+1)*batchSize,:]\n",
    "            y_batch = y[batch*batchSize:(batch+1)*batchSize]\n",
    "            loss = 0\n",
    "            for i in range(batchSize):\n",
    "                Xin = X_batch[i,:]\n",
    "                yTrue = y_batch[i]\n",
    "                y_hat = forwardStep(Xin,W_list)\n",
    "                loss += loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "            loss = loss/batchSize\n",
    "            loss.backward()\n",
    "            sys.stdout.flush()\n",
    "            dW_list = []\n",
    "            for j in range(len(W_list)):\n",
    "                dW_list.append(W_list[j].grad.data)\n",
    "            W_list = updateParams(W_list,dW_list,lr)\n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss/numBatches))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch=0: 0.715217\n",
      "Loss after epoch=1: 0.715216\n",
      "Loss after epoch=2: 0.715214\n",
      "Loss after epoch=3: 0.715212\n",
      "Loss after epoch=4: 0.715211\n",
      "Loss after epoch=5: 0.715209\n",
      "Loss after epoch=6: 0.715208\n",
      "Loss after epoch=7: 0.715206\n",
      "Loss after epoch=8: 0.715205\n",
      "Loss after epoch=9: 0.715203\n",
      "Loss after epoch=10: 0.715201\n",
      "Loss after epoch=11: 0.715200\n",
      "Loss after epoch=12: 0.715198\n",
      "Loss after epoch=13: 0.715197\n",
      "Loss after epoch=14: 0.715195\n",
      "Loss after epoch=15: 0.715194\n",
      "Loss after epoch=16: 0.715192\n",
      "Loss after epoch=17: 0.715190\n",
      "Loss after epoch=18: 0.715189\n",
      "Loss after epoch=19: 0.715187\n",
      "Loss after epoch=20: 0.715186\n",
      "Loss after epoch=21: 0.715184\n",
      "Loss after epoch=22: 0.715183\n",
      "Loss after epoch=23: 0.715181\n",
      "Loss after epoch=24: 0.715179\n",
      "Loss after epoch=25: 0.715178\n",
      "Loss after epoch=26: 0.715176\n",
      "Loss after epoch=27: 0.715175\n",
      "Loss after epoch=28: 0.715173\n",
      "Loss after epoch=29: 0.715172\n",
      "Loss after epoch=30: 0.715170\n",
      "Loss after epoch=31: 0.715168\n",
      "Loss after epoch=32: 0.715167\n",
      "Loss after epoch=33: 0.715165\n",
      "Loss after epoch=34: 0.715164\n",
      "Loss after epoch=35: 0.715162\n",
      "Loss after epoch=36: 0.715161\n",
      "Loss after epoch=37: 0.715159\n",
      "Loss after epoch=38: 0.715158\n",
      "Loss after epoch=39: 0.715156\n",
      "Loss after epoch=40: 0.715154\n",
      "Loss after epoch=41: 0.715153\n",
      "Loss after epoch=42: 0.715151\n",
      "Loss after epoch=43: 0.715150\n",
      "Loss after epoch=44: 0.715148\n",
      "Loss after epoch=45: 0.715147\n",
      "Loss after epoch=46: 0.715145\n",
      "Loss after epoch=47: 0.715143\n",
      "Loss after epoch=48: 0.715142\n",
      "Loss after epoch=49: 0.715140\n",
      "Loss after epoch=50: 0.715139\n",
      "Loss after epoch=51: 0.715137\n",
      "Loss after epoch=52: 0.715136\n",
      "Loss after epoch=53: 0.715134\n",
      "Loss after epoch=54: 0.715132\n",
      "Loss after epoch=55: 0.715131\n",
      "Loss after epoch=56: 0.715129\n",
      "Loss after epoch=57: 0.715128\n",
      "Loss after epoch=58: 0.715126\n",
      "Loss after epoch=59: 0.715125\n",
      "Loss after epoch=60: 0.715123\n",
      "Loss after epoch=61: 0.715122\n",
      "Loss after epoch=62: 0.715120\n",
      "Loss after epoch=63: 0.715118\n",
      "Loss after epoch=64: 0.715117\n",
      "Loss after epoch=65: 0.715115\n",
      "Loss after epoch=66: 0.715114\n",
      "Loss after epoch=67: 0.715112\n",
      "Loss after epoch=68: 0.715111\n",
      "Loss after epoch=69: 0.715109\n",
      "Loss after epoch=70: 0.715107\n",
      "Loss after epoch=71: 0.715106\n",
      "Loss after epoch=72: 0.715104\n",
      "Loss after epoch=73: 0.715103\n",
      "Loss after epoch=74: 0.715101\n",
      "Loss after epoch=75: 0.715100\n",
      "Loss after epoch=76: 0.715098\n",
      "Loss after epoch=77: 0.715097\n",
      "Loss after epoch=78: 0.715095\n",
      "Loss after epoch=79: 0.715093\n",
      "Loss after epoch=80: 0.715092\n",
      "Loss after epoch=81: 0.715090\n",
      "Loss after epoch=82: 0.715089\n",
      "Loss after epoch=83: 0.715087\n",
      "Loss after epoch=84: 0.715086\n",
      "Loss after epoch=85: 0.715084\n",
      "Loss after epoch=86: 0.715082\n",
      "Loss after epoch=87: 0.715081\n",
      "Loss after epoch=88: 0.715079\n",
      "Loss after epoch=89: 0.715078\n",
      "Loss after epoch=90: 0.715076\n",
      "Loss after epoch=91: 0.715075\n",
      "Loss after epoch=92: 0.715073\n",
      "Loss after epoch=93: 0.715072\n",
      "Loss after epoch=94: 0.715070\n",
      "Loss after epoch=95: 0.715068\n",
      "Loss after epoch=96: 0.715067\n",
      "Loss after epoch=97: 0.715065\n",
      "Loss after epoch=98: 0.715064\n",
      "Loss after epoch=99: 0.715062\n"
     ]
    }
   ],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputDim)),requires_grad=True)\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
    "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
    "\n",
    "W_list = []\n",
    "W_list.append(W1)\n",
    "W_list.append(W2)\n",
    "W_list.append(W3)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "#W_list = trainNN_sgd(X,y,W_list,loss_fn,lr=0.0001,nepochs=100)\n",
    "#W_list = trainNN_batch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100)\n",
    "W_list = trainNN_batch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(np.random.uniform(0,1,inputDim),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = wSum(X[0,:],W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2336, dtype=torch.float64, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5785, dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputDim)),requires_grad=True)\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
    "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
    "\n",
    "W_list = []\n",
    "W_list.append(W1)\n",
    "W_list.append(W2)\n",
    "W_list.append(W3)\n",
    "z = forwardStep(X[0,:],W_list)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss_fun = nn.BCELoss()\n",
    "lr = 0.0001\n",
    "x = torch.randn(1)\n",
    "y = torch.randint(0,2,(1,),dtype=torch.float)\n",
    "w = torch.randn(1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7737970948219299\n",
      "0.7737958431243896\n",
      "0.7737945318222046\n",
      "0.77379310131073\n",
      "0.7737917900085449\n",
      "0.7737905383110046\n",
      "0.77378910779953\n",
      "0.773787796497345\n",
      "0.7737863659858704\n",
      "0.7737851142883301\n",
      "0.773783802986145\n",
      "0.77378249168396\n",
      "0.7737810611724854\n",
      "0.7737798094749451\n",
      "0.77377849817276\n",
      "0.7737770676612854\n",
      "0.7737757563591003\n",
      "0.7737745046615601\n",
      "0.7737730741500854\n",
      "0.7737717628479004\n",
      "0.7737705111503601\n",
      "0.7737690806388855\n",
      "0.7737677693367004\n",
      "0.7737664580345154\n",
      "0.7737652063369751\n",
      "0.7737637758255005\n",
      "0.7737624645233154\n",
      "0.7737610340118408\n",
      "0.7737597823143005\n",
      "0.7737584710121155\n",
      "0.7737570405006409\n",
      "0.7737559080123901\n",
      "0.7737544775009155\n",
      "0.7737531661987305\n",
      "0.7737517356872559\n",
      "0.7737504839897156\n",
      "0.773749053478241\n",
      "0.7737477421760559\n",
      "0.7737464308738708\n",
      "0.7737451791763306\n",
      "0.773743748664856\n",
      "0.7737424373626709\n",
      "0.7737411260604858\n",
      "0.773739755153656\n",
      "0.773738443851471\n",
      "0.7737370133399963\n",
      "0.7737358808517456\n",
      "0.7737345695495605\n",
      "0.7737331390380859\n",
      "0.7737318277359009\n",
      "0.773730456829071\n",
      "0.773729145526886\n",
      "0.7737278342247009\n",
      "0.7737265825271606\n",
      "0.7737252712249756\n",
      "0.773723840713501\n",
      "0.7737225294113159\n",
      "0.7737210988998413\n",
      "0.773719847202301\n",
      "0.773718535900116\n",
      "0.7737172245979309\n",
      "0.7737159729003906\n",
      "0.773714542388916\n",
      "0.773713231086731\n",
      "0.7737118005752563\n",
      "0.7737106680870056\n",
      "0.773709237575531\n",
      "0.773707926273346\n",
      "0.7737066745758057\n",
      "0.773705244064331\n",
      "0.773703932762146\n",
      "0.7737026214599609\n",
      "0.7737013697624207\n",
      "0.7737000584602356\n",
      "0.773698627948761\n",
      "0.7736973762512207\n",
      "0.7736959457397461\n",
      "0.773694634437561\n",
      "0.773693323135376\n",
      "0.7736920714378357\n",
      "0.7736907601356506\n",
      "0.773689329624176\n",
      "0.7736880779266357\n",
      "0.7736866474151611\n",
      "0.7736853361129761\n",
      "0.773684024810791\n",
      "0.7736827731132507\n",
      "0.7736814618110657\n",
      "0.7736800312995911\n",
      "0.773678719997406\n",
      "0.7736773490905762\n",
      "0.7736761569976807\n",
      "0.773674726486206\n",
      "0.7736734747886658\n",
      "0.7736721634864807\n",
      "0.7736707329750061\n",
      "0.773669421672821\n",
      "0.7736681699752808\n",
      "0.7736668586730957\n",
      "0.7736655473709106\n"
     ]
    }
   ],
   "source": [
    "nIter = 100\n",
    "for i in range(nIter):\n",
    "    y_hat = m(w*x)\n",
    "    loss = loss_fun(y_hat,y)\n",
    "    loss.backward()\n",
    "    dw = w.grad.data\n",
    "    with torch.no_grad():\n",
    "        w -= lr*dw\n",
    "    w.grad.data.zero_()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "tensor_x = torch.Tensor(X)\n",
    "tensor_y = torch.Tensor(y)\n",
    "Xy = TensorDataset(tensor_x,tensor_y)\n",
    "Xy_loader = DataLoader(Xy,batch_size=16,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(inputDim,200),\n",
    "    nn.ReLU(),\n",
    "    #nn.BatchNorm1d(num_features=200),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(200,100),\n",
    "    nn.Tanh(),\n",
    "    #nn.BatchNorm1d(num_features=100),\n",
    "    nn.Linear(100,1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m batch_size \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m y_hat \u001b[39m=\u001b[39m model(X\u001b[39m.\u001b[39mview(batch_size,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_hat,y)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      8\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Charl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Charl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\Charl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3086\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3084\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3085\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[1;32m-> 3086\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3087\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3089\u001b[0m     )\n\u001b[0;32m   3091\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3092\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "nepochs = 100\n",
    "for epoch in range(nepochs):\n",
    "    for X,y in Xy_loader:\n",
    "        batch_size = X.shape[0]\n",
    "        y_hat = model(X.view(batch_size,-1))\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    xt = torch.tensor(np.random.rand(1,inputDim))\n",
    "    y2 = model(xt.float())\n",
    "    print(y2.detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ee6f23f9c4d80c203dea4faa6a5d0c8cffa4ab65b221251ea7227320f350d1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
